# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC üß† Este notebook foi desenvolvido pela **@Aprender Dados** e faz parte dos nossos materiais educacionais para alunos e participantes dos treinamentos.
# MAGIC
# MAGIC üìò O conte√∫do aqui apresentado pode ser utilizado livremente para fins de estudo. Caso voc√™ tenha recebido este notebook de terceiros, saiba que ele √© parte de um curso completo com v√≠deo-aulas, exerc√≠cios guiados e suporte da comunidade.
# MAGIC
# MAGIC üöÄ Quer aprender mais e se tornar um engenheiro de dados profissional?  
# MAGIC
# MAGIC
# MAGIC üëâ [Conhe√ßa nossos treinamentos](https://pay.kiwify.com.br/4OxeVMk)
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/AprenderDados/public-files/main/public-files/ad/roadmap_databricks.jpg" alt="Aprender Dados" width="1080"/>
# MAGIC
# MAGIC
# MAGIC ---
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC # üìÖ Aula 01 ‚Äì Revis√£o Associate
# MAGIC
# MAGIC Nesta aula, vamos revisar os principais conceitos da certifica√ß√£o Databricks Associate e iniciar nosso curso com exemplos pr√°ticos, conectando teoria com execu√ß√£o.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### ‚úÖ O que vamos abordar:
# MAGIC - Revis√£o Spark, Delta Lake e Workflows
# MAGIC - Comandos SQL e PySpark com Delta Tables
# MAGIC - Demonstra√ß√£o pr√°tica do ACID
# MAGIC - Time Travel, ZORDER e Particionamento
# MAGIC - Setup para execu√ß√£o dos cadernos
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **Dica:** Voc√™ pode executar este caderno usando um cluster Serverless gratuito no Databricks Free.
# MAGIC
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üìñ Refer√™ncias:
# MAGIC - [Databricks Certification Guide](https://www.databricks.com/learn/certification)
# MAGIC - [Documenta√ß√£o Delta Lake](https://docs.databricks.com/delta/index.html)
# MAGIC - [Spark Programming Guide](https://spark.apache.org/docs/latest/)
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC # üß† Delta Lake e Transa√ß√µes ACID
# MAGIC
# MAGIC O Delta Lake √© a tecnologia de armazenamento transacional utilizada no Databricks.
# MAGIC
# MAGIC Nesta aula, voc√™ vai:
# MAGIC - Criar e manipular tabelas Delta com SQL e PySpark
# MAGIC - Entender na pr√°tica os conceitos de ACID
# MAGIC - Explorar o Delta Log
# MAGIC - Comparar tabelas gerenciadas (managed) vs externas
# MAGIC - Ver como o `DROP` e o `DELETE` afetam os dados

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE SCHEMA IF NOT EXISTS workspace.demo;

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## ‚ú® Revis√£o: Lakehouse + Spark + Delta Lake
# MAGIC
# MAGIC O modelo Lakehouse combina a escalabilidade dos data lakes com a confiabilidade dos data warehouses. 
# MAGIC Delta Lake √© a tecnologia que habilita isso, trazendo:
# MAGIC - Transa√ß√µes ACID
# MAGIC - Controle de vers√£o
# MAGIC - Performance (ZORDER, OPTIMIZE, Particionamento)
# MAGIC - Time Travel
# MAGIC
# MAGIC Spark √© o motor de processamento distribu√≠do. Suporta SQL, Python, R, Scala e Java.
# MAGIC
# MAGIC ### ‚ö° Demonstra√ß√£o: Criando Tabela Delta
# MAGIC
# MAGIC Vamos criar uma tabela Delta fict√≠cia com dados de vendas:

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## ‚ú≥Ô∏è Criando uma tabela Delta com SQL e PySpark
# MAGIC
# MAGIC Vamos criar uma tabela simples no Delta Lake com dados fict√≠cios.

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Cria a tabela Delta com os dados
# MAGIC CREATE OR REPLACE TABLE workspace.demo.tabela_usuarios (
# MAGIC   nome STRING,
# MAGIC   idade INT
# MAGIC ) USING DELTA;
# MAGIC
# MAGIC -- Insere os dados
# MAGIC INSERT INTO workspace.demo.tabela_usuarios (nome, idade) VALUES
# MAGIC   ('Alice', 30),
# MAGIC   ('Bob', 25),
# MAGIC   ('Carol', 40);

# COMMAND ----------

from pyspark.sql.types import IntegerType

dados = [("Alice", 30), ("Bob", 25), ("Carol", 40)]
colunas = ["nome", "idade"]

df = spark.createDataFrame(dados, colunas)
df = df.withColumn("idade", df["idade"].cast(IntegerType()))

df.write.format("delta") \
  .mode("overwrite") \
  .saveAsTable("workspace.demo.tabela_usuarios")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM workspace.demo.tabela_usuarios;

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üîÅ Inserindo e Atualizando com SQL
# MAGIC
# MAGIC Vamos adicionar novos registros e atualizar um deles.

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- INSERT
# MAGIC INSERT INTO workspace.demo.tabela_usuarios VALUES
# MAGIC   ("Daniel", 22),
# MAGIC   ("Eva", 33);
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC -- UPDATE
# MAGIC UPDATE workspace.demo.tabela_usuarios
# MAGIC SET idade = 28
# MAGIC WHERE nome = "Bob";

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM workspace.demo.tabela_usuarios;

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üß™ Transa√ß√µes ACID em A√ß√£o
# MAGIC
# MAGIC ### Conceito:
# MAGIC ACID = **Atomicidade, Consist√™ncia, Isolamento, Durabilidade**
# MAGIC
# MAGIC Delta Lake garante:
# MAGIC - **Atomicidade**: ou tudo ou nada
# MAGIC - **Consist√™ncia**: dados v√°lidos
# MAGIC - **Isolamento**: uma transa√ß√£o n√£o interfere em outra
# MAGIC - **Durabilidade**: os dados s√£o persistidos mesmo ap√≥s falhas
# MAGIC
# MAGIC
# MAGIC Vamos simular um erro para observar o comportamento de rollback.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### ‚öóÔ∏è A ‚Äî Atomicidade
# MAGIC
# MAGIC Uma opera√ß√£o √© toda ou nada. Se falhar no meio, nada √© aplicado.
# MAGIC

# COMMAND ----------

try:
    # Simulando erro no meio da escrita
    df_error = spark.createDataFrame([("Zoe", "idade_invalida")], ["nome", "idade"])
    df_error.write.mode("append").saveAsTable("workspace.demo.tabela_usuarios")
except Exception as e:
    print(f"Erro capturado: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC A tabela continua √≠ntegra mesmo ap√≥s erro!  
# MAGIC üîí Isso acontece por causa das transa√ß√µes ACID do Delta Lake.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### ‚úÖ C ‚Äî Consist√™ncia
# MAGIC
# MAGIC Os dados respeitam o esquema e regras da tabela.
# MAGIC
# MAGIC Vamos tentar escrever um DataFrame com colunas em ordem errada.
# MAGIC

# COMMAND ----------

df_invalido = spark.createDataFrame([(32, "Jo√£o")], ["nome", "idade"])
df_valido = spark.createDataFrame([(32, "Jo√£o")], ["idade", "nome"])

try:
    df_invalido.write.mode("append").format("delta").saveAsTable("workspace.demo.tabela_usuarios")
    #df_valido.write.mode("append").format("delta").saveAsTable("workspace.demo.tabela_usuarios")
except Exception as e:
    print(f"üö´ Esquema inv√°lido: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üîí I ‚Äî Isolamento (Concorr√™ncia)
# MAGIC
# MAGIC Delta Lake garante que m√∫ltiplas transa√ß√µes paralelas n√£o corrompam os dados.
# MAGIC
# MAGIC Vamos simular m√∫ltiplos `append` ao mesmo tempo.
# MAGIC

# COMMAND ----------

from threading import Thread

def insert_nome(nome):
    df = spark.createDataFrame([(nome, 99)], ["nome", "idade"])
    df.write.mode("append").format("delta").saveAsTable("workspace.demo.tabela_usuarios")

# Criar threads concorrentes
t1 = Thread(target=insert_nome, args=("Concorrente_1",))
t2 = Thread(target=insert_nome, args=("Concorrente_2",))

t1.start()
t2.start()
t1.join()
t2.join()


# COMMAND ----------

# MAGIC %sql
# MAGIC -- Verificar se os dois inserts ocorreram corretamente
# MAGIC SELECT * FROM workspace.demo.tabela_usuarios;
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üíæ D ‚Äî Durabilidade
# MAGIC
# MAGIC Os dados s√£o preservados mesmo ap√≥s falhas.
# MAGIC
# MAGIC Reiniciar o cluster ou simular uma falha de sistema **n√£o** afeta os dados persistidos no Delta Lake.
# MAGIC
# MAGIC Vamos ver o hist√≥rico de transa√ß√µes:
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE HISTORY workspace.demo.tabela_usuarios;
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üìÅ Entendendo o Delta Log
# MAGIC
# MAGIC Cada tabela Delta mant√©m um hist√≥rico de mudan√ßas. Vamos ver isso:

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üß¨ Estrutura de Arquivos no Delta Lake
# MAGIC
# MAGIC Ao salvar uma tabela Delta particionada, os dados s√£o armazenados em **pastas por parti√ß√£o** e o Delta mant√©m um controle completo de todas as altera√ß√µes em uma pasta chamada `_delta_log`.
# MAGIC
# MAGIC ### üìÅ Exemplo: Estrutura da Tabela Particionada por `ano_mes`
# MAGIC
# MAGIC /Volumes/main/default/minicurso/tabela_vendas_zorder/  
# MAGIC
# MAGIC - `/Volumes/main/default/minicurso/tabela_vendas_zorder/`
# MAGIC   - `_delta_log/`
# MAGIC     - `00000000000000000000.json`
# MAGIC     - `00000000000000000001.json`
# MAGIC     - `...` *(arquivos de log que registram cada transa√ß√£o)*
# MAGIC   - `ano_mes=2022-01/`
# MAGIC     - `part-00000-*.snappy.parquet`
# MAGIC     - `...`
# MAGIC   - `ano_mes=2022-02/`
# MAGIC     - `part-00000-*.snappy.parquet`
# MAGIC     - `...`
# MAGIC   - `ano_mes=2023-01/`
# MAGIC     - `part-00000-*.snappy.parquet`
# MAGIC     - `...`
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üß± _delta_log: Como funciona?
# MAGIC
# MAGIC A pasta `_delta_log` registra **tudo que acontece na tabela**:  
# MAGIC - Cada arquivo `.json` representa uma transa√ß√£o (append, update, delete, optimize‚Ä¶)  
# MAGIC - O Delta usa isso para garantir **ACID** e **Time Travel**  
# MAGIC - Os arquivos `.checkpoint.parquet` (n√£o vis√≠veis aqui) ajudam a acelerar a leitura do log  
# MAGIC
# MAGIC ### üß† Curiosidade
# MAGIC
# MAGIC Mesmo ap√≥s um `DELETE` ou `UPDATE`, os dados ainda est√£o armazenados no Delta, at√© que voc√™ fa√ßa um `VACUUM`.  
# MAGIC Isso permite **Time Travel** e **rollback seguro**!
# MAGIC
# MAGIC üîç Quer ver isso na pr√°tica?  
# MAGIC Tente rodar:
# MAGIC
# MAGIC ```sql
# MAGIC DESCRIBE HISTORY delta.`/Volumes/main/default/minicurso/tabela_vendas_zorder`;

# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE HISTORY workspace.demo.tabela_usuarios;

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üÜö Tabelas Managed vs External
# MAGIC
# MAGIC - **Managed**: o Databricks gerencia os arquivos (padr√£o do `saveAsTable`)
# MAGIC - **External**: voc√™ define o caminho do armazenamento (ideal com Unity Catalog)
# MAGIC
# MAGIC ### Criando tabela Managed

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- Removendo uma linha
# MAGIC DELETE FROM workspace.demo.tabela_usuarios WHERE nome = "Alice";
# MAGIC
# MAGIC -- Drop completo da tabela
# MAGIC DROP TABLE IF EXISTS workspace.demo.tabela_usuarios;

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## ‚öîÔ∏è Concurrency Control com Delta Lake
# MAGIC
# MAGIC Uma das vantagens do Delta Lake √© permitir m√∫ltiplas grava√ß√µes simult√¢neas com seguran√ßa.  
# MAGIC Vamos simular m√∫ltiplas inser√ß√µes e garantir que a consist√™ncia seja mantida.

# COMMAND ----------

# Simulando m√∫ltiplas inser√ß√µes r√°pidas
from time import time
from pyspark.sql.functions import lit

for i in range(10):
    df_novo = spark.createDataFrame([("User_" + str(i), 20 + i)], ["nome", "idade"])
    df_novo = df_novo.withColumn("insercao_id", lit(i))
    df_novo.write.mode("append").saveAsTable("workspace.demo.tabela_concorrencia")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM workspace.demo.tabela_concorrencia;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(*) AS total_linhas, MAX(insercao_id) AS ultima_insercao
# MAGIC FROM workspace.demo.tabela_concorrencia;

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üï∞Ô∏è Time Travel com Delta Lake
# MAGIC
# MAGIC Podemos viajar no tempo para uma vers√£o anterior da tabela.  
# MAGIC Use `DESCRIBE HISTORY` para descobrir vers√µes anteriores.

# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE HISTORY workspace.demo.tabela_concorrencia;

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Visualizar uma vers√£o antiga (exemplo: 0)
# MAGIC SELECT * FROM workspace.demo.tabela_concorrencia VERSION AS OF 0;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM workspace.demo.tabela_concorrencia@v9

# COMMAND ----------

data = spark.table("workspace.demo.tabela_concorrencia@v9").display()

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## ‚öôÔ∏è Otimiza√ß√µes com OPTIMIZE e ZORDER
# MAGIC
# MAGIC No Delta Lake, o comando `OPTIMIZE` compacta pequenos arquivos.  
# MAGIC Com `ZORDER`, otimizamos a leitura por colunas mais usadas em filtros.

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Otimizando a tabela para reduzir arquivos pequenos
# MAGIC OPTIMIZE workspace.demo.tabela_concorrencia;

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- Z-Ordering pela coluna mais filtrada
# MAGIC OPTIMIZE workspace.demo.tabela_concorrencia ZORDER BY (nome);

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üß± Particionamento
# MAGIC
# MAGIC O particionamento melhora a leitura em grandes tabelas ao dividir os dados fisicamente com base em uma coluna.  
# MAGIC Isso permite que o mecanismo de execu√ß√£o leia apenas as parti√ß√µes necess√°rias para responder √† consulta, economizando tempo e recursos.
# MAGIC
# MAGIC Vamos comparar:
# MAGIC
# MAGIC - Uma tabela sem particionamento
# MAGIC - Uma tabela com particionamento por `ano_mes`
# MAGIC
# MAGIC E verificar como os arquivos s√£o organizados no Volume.
# MAGIC
# MAGIC ---

# COMMAND ----------

from pyspark.sql.functions import col, rand, expr, when, date_format
from datetime import datetime, timedelta
import random

# Categorias e datas variadas para gerar m√∫ltiplas parti√ß√µes
categorias = ["Eletr√¥nicos", "Roupas", "Alimentos", "Livros", "Brinquedos"]
datas = [f"2023-{str(m).zfill(2)}-01" for m in range(1, 13)]  # 12 meses

# Gerando 1 milh√£o de registros distribu√≠dos por categorias e meses
df = spark.range(1_000_000_000).selectExpr("id as dummy") \
    .withColumn("data_venda", expr(f"date('{random.choice(datas)}')")) \
    .withColumn("categoria", when(rand() < 0.2, categorias[0])
                             .when(rand() < 0.4, categorias[1])
                             .when(rand() < 0.6, categorias[2])
                             .when(rand() < 0.8, categorias[3])
                             .otherwise(categorias[4])) \
    .withColumn("valor", (rand() * 1000).cast("double")) \
    .withColumn("ano_mes", date_format("data_venda", "yyyy-MM")) \
    .drop("dummy")

df.display()

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE VOLUME IF NOT EXISTS workspace.demo.com_particao;
# MAGIC CREATE VOLUME IF NOT EXISTS workspace.demo.sem_particao;

# COMMAND ----------

# Caminhos para salvar
caminho_sem_particao = "/Volumes/workspace/demo/sem_particao"
caminho_com_particao = "/Volumes/workspace/demo/com_particao"

# COMMAND ----------

# Salvando SEM particionamento
df.write.format("delta").mode("overwrite").save(caminho_sem_particao)

# COMMAND ----------

# Salvando COM particionamento
df.write.format("delta").mode("overwrite").partitionBy("ano_mes").save(caminho_com_particao)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üîç Explorando as pastas geradas
# MAGIC
# MAGIC No Databricks, podemos usar comandos `ls` no caminho do volume para observar a diferen√ßa:

# COMMAND ----------

# Lista da tabela sem particionamento
display(dbutils.fs.ls(caminho_sem_particao))

# COMMAND ----------

# Lista da tabela com particionamento
display(dbutils.fs.ls(caminho_com_particao))

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üìä Consulta com filtro
# MAGIC
# MAGIC A consulta com filtro funciona de maneira igual, mas a leitura ser√° muito mais eficiente na vers√£o particionada.

# COMMAND ----------

from time import time

print("Consulta SEM particionamento")
start = time()
spark.read.format("delta").load(caminho_sem_particao).filter("ano_mes = '2023-03'").count()
print(f"Tempo: {time() - start:.2f} segundos")

print("\nConsulta COM particionamento")
start = time()
spark.read.format("delta").load(caminho_com_particao).filter("ano_mes = '2023-03'").count()
print(f"Tempo: {time() - start:.2f} segundos")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÅ Experimento com m√∫ltiplos appends (Auto Optimize desativado)
# MAGIC
# MAGIC Vamos criar uma tabela Delta externa no volume `main.default`, gerar v√°rios appends com 1 milh√£o de linhas por itera√ß√£o, e medir o impacto da fragmenta√ß√£o.
# MAGIC
# MAGIC No final, aplicaremos `OPTIMIZE` + `ZORDER BY (categoria)` e compararemos os planos com `.explain()`.
# MAGIC
# MAGIC Auto Optimize ser√° desativado para simular fragmenta√ß√£o real.

# COMMAND ----------

# MAGIC %md
# MAGIC ### üìä Gerar dados sint√©ticos e criar tabela Delta externa
# MAGIC Vamos gerar 10 milh√µes de registros de vendas, particionados por `ano_mes`, e salvar como tabela Delta.

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE VOLUME IF NOT EXISTS workspace.demo.tabela_vendas_zorder;

# COMMAND ----------

# MAGIC %sql
# MAGIC DROP TABLE IF EXISTS delta.`/Volumes/workspace/demo/tabela_vendas_zorder` 

# COMMAND ----------

dbutils.fs.rm("/Volumes/workspace/demo/tabela_vendas_zorder", recurse=True)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE OR REPLACE TABLE delta.`/Volumes/workspace/demo/tabela_vendas_zorder` (
# MAGIC   data_venda DATE,
# MAGIC   categoria STRING,
# MAGIC   valor DOUBLE,
# MAGIC   ano_mes STRING
# MAGIC )
# MAGIC USING DELTA
# MAGIC PARTITIONED BY (ano_mes)
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TBLPROPERTIES delta.`/Volumes/workspace/demo/tabela_vendas_zorder`;

# COMMAND ----------

# MAGIC %sql
# MAGIC ALTER TABLE delta.`/Volumes/workspace/demo/tabela_vendas_zorder`
# MAGIC SET TBLPROPERTIES (
# MAGIC   'delta.autoOptimize.optimizeWrite' = 'false',
# MAGIC   'delta.autoOptimize.autoCompact' = 'false'
# MAGIC );

# COMMAND ----------

from pyspark.sql.functions import col, rand, expr, when, floor
from datetime import datetime
import random

# Categorias e intervalo de datas
categorias = ["Eletr√¥nicos", "Roupas", "Alimentos", "Livros", "Brinquedos"]
data_inicio = datetime.strptime("2022-01-01", "%Y-%m-%d")
data_fim = datetime.strptime("2023-12-31", "%Y-%m-%d")
dias_total = (data_fim - data_inicio).days

# Express√£o condicional para categorias baseada em rand()
categoria_expr = when(rand() < 0.2, categorias[0]) \
    .when(rand() < 0.4, categorias[1]) \
    .when(rand() < 0.6, categorias[2]) \
    .when(rand() < 0.8, categorias[3]) \
    .otherwise(categorias[4])

# Loop para gerar m√∫ltiplos appends
for i in range(200):
    print(f"Append {i+1}...")

    df_append = spark.range(1_000_000).selectExpr("id as dummy") \
        .withColumn("rand_days", floor(rand() * dias_total).cast("int")) \
        .withColumn("data_venda", expr(f"date_add('{data_inicio.strftime('%Y-%m-%d')}', rand_days)")) \
        .withColumn("categoria", categoria_expr) \
        .withColumn("valor", (rand() * 1000).cast("double")) \
        .withColumn("ano_mes", expr("date_format(data_venda, 'yyyy-MM')")) \
        .drop("dummy", "rand_days")

    df_append.write.format("delta").mode("append") \
        .partitionBy("ano_mes") \
        .save("/Volumes/workspace/demo/tabela_vendas_zorder")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üß™ Consulta antes da otimiza√ß√£o
# MAGIC
# MAGIC Vamos consultar um valor espec√≠fico (ex: `estado = 'SP'`) e usar `.explain()` para entender o plano de execu√ß√£o atual.

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Consulta buscando uma categoria espec√≠fica (sem otimiza√ß√£o ainda)
# MAGIC SELECT *
# MAGIC FROM delta.`/Volumes/workspace/demo/tabela_vendas_zorder`
# MAGIC WHERE categoria = 'Eletr√¥nicos'

# COMMAND ----------

# MAGIC %sql
# MAGIC -- EXPLAIN antes da otimiza√ß√£o
# MAGIC EXPLAIN FORMATTED
# MAGIC SELECT *
# MAGIC FROM delta.`/Volumes/workspace/demo/tabela_vendas_zorder`
# MAGIC WHERE categoria = 'Eletr√¥nicos'

# COMMAND ----------

# Medindo tempo ANTES do OPTIMIZE com agrega√ß√£o for√ßando leitura ampla
import time
from pyspark.sql.functions import * 

start = time.time()
df = spark.read.format("delta").load("/Volumes/workspace/demo/tabela_vendas_zorder")

# Agrega√ß√£o por categoria
resultado = df.groupBy("categoria").agg(
    count("*").alias("total_vendas"),
    round(sum("valor"), 2).alias("total_valor")
).collect()

end = time.time()

print(f"Tempo de execu√ß√£o (antes do OPTIMIZE): {end - start:.2f} segundos")
display(resultado)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### ‚öôÔ∏è Aplicando OPTIMIZE + ZORDER
# MAGIC
# MAGIC Vamos compactar os arquivos e aplicar ordena√ß√£o por `estado`, que √© a coluna mais usada em filtros.

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Aplicando OPTIMIZE com ZORDER BY na coluna "categoria"
# MAGIC OPTIMIZE delta.`/Volumes/workspace/demo/tabela_vendas_zorder`
# MAGIC ZORDER BY (categoria)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### üß™ Consulta ap√≥s da otimiza√ß√£o

# COMMAND ----------

# MAGIC %sql
# MAGIC -- EXPLAIN depois da otimiza√ß√£o
# MAGIC EXPLAIN FORMATTED
# MAGIC SELECT *
# MAGIC FROM delta.`/Volumes/workspace/demo/tabela_vendas_zorder`
# MAGIC WHERE categoria = 'Eletr√¥nicos'

# COMMAND ----------

# Medindo tempo DEPOIS do OPTIMIZE com agrega√ß√£o for√ßando leitura ampla
import time
from pyspark.sql.functions import * 

start = time.time()
df = spark.read.format("delta").load("/Volumes/workspace/demo/tabela_vendas_zorder")

# Agrega√ß√£o por categoria
resultado = df.groupBy("categoria").agg(
    count("*").alias("total_vendas"),
    round(sum("valor"), 2).alias("total_valor")
).collect()

end = time.time()

print(f"Tempo de execu√ß√£o (depois do OPTIMIZE): {end - start:.2f} segundos")
display(resultado)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üîÑ DEMO End-to-End + Setup
# MAGIC
# MAGIC - Rodar bronze, silver e gold com dados do AdventureWorks
# MAGIC - Usar workflows
# MAGIC - Simular novos dados chegando
# MAGIC - Atualizar as tabelas

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## üíª Setup do Ambiente
# MAGIC
# MAGIC ### Databricks Free
# MAGIC - Crie conta: https://www.databricks.com/learn/free-edition
# MAGIC - Crie cluster: `Serverless`
# MAGIC - Importe o projeto
# MAGIC
# MAGIC
# MAGIC ### Azure
# MAGIC - Configure Unity Catalog (ver aula dedicada)
# MAGIC - Crie cluster com UC habilitado
# MAGIC - Monte os dados no DBFS ou no ADLS
# MAGIC
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC [üîó Link para o guia de estudos no GitHub:](https://github.com/AprenderDados/quero_aprender_dados/blob/main/Aprendendo_Databricks/guia_de_estudos_certificacao_databricks.md)
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Links e Refer√™ncias
# MAGIC
# MAGIC - üîó [Documenta√ß√£o Oficial do Delta Lake (Databricks)](https://docs.databricks.com/delta/index.html)
# MAGIC - üîó [What is Delta Lake? | Databricks](https://www.databricks.com/discover/delta-lake)
# MAGIC - üîó [ACID Transactions in Delta Lake](https://docs.databricks.com/delta/delta-acid.html)
# MAGIC - üîó [Time Travel with Delta Lake](https://docs.databricks.com/delta/delta-time-travel.html)
# MAGIC - üîó [Z-Ordering para otimiza√ß√£o de leitura](https://docs.databricks.com/delta/optimize.html#z-order-by-multidimensional-clustering)
# MAGIC - üîó [Unity Catalog - Vis√£o Geral](https://docs.databricks.com/data-governance/unity-catalog/index.html)
# MAGIC - üîó [Databricks Lakehouse Architecture](https://www.databricks.com/solutions/data-lakehouse)
# MAGIC - üîó [Exemplos e tutoriais da Databricks Academy](https://academy.databricks.com)
# MAGIC
# MAGIC Essas refer√™ncias ajudam a consolidar os conceitos apresentados na aula e trazem caminhos para aprofundamento t√©cnico e pr√°tico sobre Delta Lake, arquitetura Lakehouse e pr√°ticas recomendadas em projetos com Databricks.

# COMMAND ----------


