# Databricks notebook source
print("carregando funÃ§Ãµes")

# COMMAND ----------

from pyspark.sql import DataFrame, Window
from pyspark.sql import functions as F
from pyspark.sql.types import (StructType, StructField,
        IntegerType, StringType, DoubleType, DecimalType, TimestampType, ShortType)


# COMMAND ----------



def _validate_schema(df: DataFrame, expected_schema: StructType) -> bool:
    """
    Valida se o schema do DataFrame corresponde ao schema esperado.

    ParÃ¢metros:
        df (DataFrame): O DataFrame a ser validado.
        expected_schema (StructType): O schema esperado.

    Retorna:
        bool: True se o schema corresponder, False caso contrÃ¡rio.
    """
    actual_schema = df.schema

    # Verifica se o nÃºmero de campos corresponde
    if len(expected_schema.fields) != len(actual_schema.fields):
        return False

    # Verifica cada campo e tipo de dado
    for i, field in enumerate(actual_schema.fields):
        expected_field = expected_schema.fields[i]
        if field.name != expected_field.name or not isinstance(field.dataType, type(expected_field.dataType)):
            print(f"DiscrepÃ¢ncia encontrada na coluna: {field.name}")
            print(f"Esperado: {expected_field}, Encontrado: {field}")
            return False

    return True

print("Carregada a FunÃ§Ã£o: _validate_schema(df: DataFrame, expected_schema: StructType) ")



# COMMAND ----------

# from delta.tables import DeltaTable
# from pyspark.sql import DataFrame, functions as F
# from pyspark.sql.utils import AnalysisException

# def _upsert_silver_table(
#     transformed_df: DataFrame, 
#     target_table: str, 
#     primary_keys: list, 
#     not_matched_by_source_action: str = None, 
#     not_matched_by_source_condition: str = None
# ) -> None:
#     """
#     Realiza o upsert (update e insert) na tabela Delta da camada prata,
#     suportando a evoluÃ§Ã£o do esquema e construindo dinamicamente a condiÃ§Ã£o de merge.

#     Melhorias:
#     - Verifica se a tabela existe e se Ã© uma tabela Delta antes de criar
#     - Trata o caso onde a pasta existe, mas nÃ£o Ã© Delta
#     - Adiciona logging para ajudar no troubleshooting

#     ParÃ¢metros:
#         transformed_df (DataFrame): DataFrame contendo os dados transformados para inserÃ§Ã£o na camada prata.
#         target_table (str): Nome da tabela de destino (ex: "adventure_works_silver.person_address").
#         primary_keys (list): Lista de chaves primÃ¡rias para o merge.
#         not_matched_by_source_action (str, opcional): AÃ§Ã£o para linhas sem correspondÃªncia na origem ("DELETE" ou "UPDATE").
#         not_matched_by_source_condition (str, opcional): CondiÃ§Ã£o adicional para aplicar a aÃ§Ã£o definida em not_matched_by_source_action.
#     """

#     try:
#         # Verificar se a tabela existe no catÃ¡logo do Databricks
#         if not spark.catalog.tableExists(target_table):
#             print(f"A tabela {target_table} nÃ£o existe no catÃ¡logo. Verificando o diretÃ³rio de armazenamento...")

#             # Obter o caminho fÃ­sico da tabela no DBFS
#             table_location = f"dbfs:/user/hive/warehouse/{target_table.replace('.', '/')}"
            
#             # Verificar se a pasta existe
#             if not dbutils.fs.ls(table_location):
#                 print(f"DiretÃ³rio {table_location} nÃ£o existe. Criando a tabela...")
#                 transformed_df.write.format("delta").saveAsTable(target_table)
#                 print(f"Tabela {target_table} criada com sucesso.")
#                 return
#             else:
#                 print(f"O diretÃ³rio {table_location} jÃ¡ existe. Verificando se Ã© uma tabela Delta...")

#                 try:
#                     # Tentar carregar a tabela Delta
#                     DeltaTable.forPath(spark, table_location)
#                     print(f"O diretÃ³rio {table_location} contÃ©m uma tabela Delta vÃ¡lida. Registrando no catÃ¡logo...")
                    
#                     # Criar uma nova entrada no catÃ¡logo apontando para essa pasta
#                     spark.sql(f"CREATE TABLE {target_table} USING DELTA LOCATION '{table_location}'")
#                     print(f"Tabela {target_table} registrada no catÃ¡logo com sucesso.")
                
#                 except:
#                     print(f"ERRO: O diretÃ³rio {table_location} contÃ©m arquivos, mas nÃ£o sÃ£o uma tabela Delta.")
#                     raise Exception(f"Erro ao criar a tabela {target_table}: O diretÃ³rio contÃ©m arquivos invÃ¡lidos.")

#         # Construir a condiÃ§Ã£o de merge com base nas chaves primÃ¡rias
#         merge_condition = " AND ".join([f"s.{key} = t.{key}" for key in primary_keys])

#         # Carregar a tabela Delta existente
#         delta_table = DeltaTable.forName(spark, target_table)

#         # Construir a operaÃ§Ã£o de merge
#         merge_builder = delta_table.alias("t").merge(
#             transformed_df.alias("s"),
#             merge_condition
#         ).whenMatchedUpdateAll().whenNotMatchedInsertAll()

#         # Se for necessÃ¡rio excluir registros nÃ£o encontrados na origem
#         if not_matched_by_source_action and not_matched_by_source_action.upper() == "DELETE":
#             unmatched_rows = delta_table.toDF().alias("t").join(
#                 transformed_df.alias("s"),
#                 on=[F.col(f"t.{key}") == F.col(f"s.{key}") for key in primary_keys],
#                 how="left_anti"
#             )

#             if not_matched_by_source_condition:
#                 unmatched_rows = unmatched_rows.filter(not_matched_by_source_condition)

#                 unmatched_rows.alias("s"),
#                 merge_condition
#             ).whenMatchedDelete().execute()

#         # Executar o merge
#         merge_builder.execute()
        
#         print(f"Upsert executado com sucesso para {target_table}.")
    
#     except AnalysisException as e:
#         print(f"Erro de anÃ¡lise ao processar {target_table}: {str(e)}")
#         raise
#     except Exception as e:
#         print(f"Erro inesperado ao processar {target_table}: {str(e)}")
#         raise





# COMMAND ----------

from delta.tables import DeltaTable
from pyspark.sql import DataFrame, functions as F
from pyspark.sql.utils import AnalysisException

def _upsert_silver_table(
    transformed_df: DataFrame, 
    target_table: str, 
    primary_keys: list, 
    not_matched_by_source_action: str = None, 
    not_matched_by_source_condition: str = None
) -> None:

    print(f"ğŸ”„ Iniciando upsert para {target_table}...")

    # Caminho fÃ­sico no DBFS
    db_location = "dbfs:/user/hive/warehouse/adventure_works_silver.db"
    table_name = target_table.split('.')[-1]  # Nome da tabela sem o schema
    table_location = f"{db_location}/{table_name.lower()}"  # Evita problemas de case-sensitive
    
    try:
        # Verificar se a tabela existe no catÃ¡logo
        if not spark.catalog.tableExists(target_table):
            print(f"âš ï¸ Tabela {target_table} nÃ£o existe no catÃ¡logo. Verificando diretÃ³rio {table_location}...")

            try:
                # Verificar se o diretÃ³rio jÃ¡ existe no DBFS
                if dbutils.fs.ls(table_location):
                    print(f"ğŸ“‚ O diretÃ³rio {table_location} existe. Verificando se Ã© uma tabela Delta...")

                    try:
                        DeltaTable.forPath(spark, table_location)
                        print(f"âœ… O diretÃ³rio contÃ©m uma tabela Delta vÃ¡lida. Registrando no catÃ¡logo...")
                        spark.sql(f"CREATE TABLE {target_table} USING DELTA LOCATION '{table_location}'")
                        print(f"âœ… Tabela {target_table} registrada no catÃ¡logo.")
                    except:
                        print(f"ğŸš¨ ERRO: O diretÃ³rio {table_location} contÃ©m arquivos invÃ¡lidos! Removendo a pasta...")
                        dbutils.fs.rm(table_location, recurse=True)
                        print(f"ğŸ—‘ï¸ DiretÃ³rio {table_location} removido. Criando a tabela do zero...")
                        transformed_df.write.format("delta").saveAsTable(target_table)
                        print(f"ğŸ‰ Tabela {target_table} criada com sucesso.")
                        return
                else:
                    print(f"ğŸ“‚ DiretÃ³rio {table_location} nÃ£o encontrado. Criando a tabela do zero...")
                    transformed_df.write.format("delta").saveAsTable(target_table)
                    print(f"ğŸ‰ Tabela {target_table} criada com sucesso.")
                    return
            except:
                print(f"ğŸ“‚ O diretÃ³rio {table_location} nÃ£o existe. Criando a tabela...")
                transformed_df.write.format("delta").saveAsTable(target_table)
                print(f"ğŸ‰ Tabela {target_table} criada com sucesso.")
                return

        else:
            print(f"âœ… A tabela {target_table} jÃ¡ existe. Usando OVERWRITE para substituir...")

            # Sobrescreve a tabela existente
            transformed_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_table)
            print(f"ğŸ‰ Tabela {target_table} substituÃ­da com sucesso.")

    except AnalysisException as e:
        print(f"ğŸš¨ Erro ao processar {target_table}: {str(e)}")

        if "TABLE_OR_VIEW_ALREADY_EXISTS" in str(e) or "not a Delta table" in str(e):
            print(f"ğŸ—‘ï¸ Deletando diretÃ³rio {table_location} e recriando a tabela...")

            # Deletar a pasta e recriar a tabela do zero
            dbutils.fs.rm(table_location, recurse=True)
            spark.sql(f"DROP TABLE IF EXISTS {target_table}")

            transformed_df.write.format("delta").saveAsTable(target_table)
            print(f"ğŸ‰ Tabela {target_table} recriada com sucesso.")

        else:
            raise  # Levanta o erro se nÃ£o for problema de tabela jÃ¡ existente ou arquivos invÃ¡lidos

    except Exception as e:
        print(f"ğŸš¨ Erro inesperado ao processar {target_table}: {str(e)}")
        raise


# COMMAND ----------

print("FunÃ§Ã£o _upsert_silver_table carregada com melhorias para tratamento de erros.")

# COMMAND ----------


